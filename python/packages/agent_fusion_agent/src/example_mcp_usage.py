"""MCPé›†æˆä½¿ç”¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„MCPé›†æˆåŠŸèƒ½å’Œhandle_responseæœºåˆ¶ã€‚
"""

import asyncio
from mcp import StdioServerParameters
from agent import create_simple_agent, create_mcp_agent, AgentConfig, SimpleAgent, MCPMixin
from llm_client import MockLLMClient, get_llm_client_manager
from observability import get_observability_manager


async def example_simple_agent_with_mcp():
    """ç¤ºä¾‹ï¼šåˆ›å»ºå¸¦æœ‰MCPå·¥å…·çš„SimpleAgent"""
    print("=== ç¤ºä¾‹ï¼šSimpleAgent with MCPå·¥å…· ===")
    
    # åˆ›å»ºMCPå·¥å…·é…ç½®
    mcp_tools = [
        StdioServerParameters(
            command="python",
            args=["-c", "import sys; print('Hello from Python tool!')"],
            env={"PYTHONPATH": "."}
        )
    ]
    
    # è®¾ç½®LLMå®¢æˆ·ç«¯
    llm_manager = get_llm_client_manager()
    mock_client = MockLLMClient()
    llm_manager.register_client("example_client", mock_client)
    
    # åˆ›å»ºAgent - æ–¹å¼1ï¼šä½¿ç”¨ä¾¿åˆ©å‡½æ•°
    agent1 = create_simple_agent(
        name="MCPåŠ©æ‰‹",
        model="gpt-3.5-turbo",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªæ‹¥æœ‰å·¥å…·èƒ½åŠ›çš„AIåŠ©æ‰‹ã€‚",
        mcp_tools=mcp_tools,
        llm_client_name="example_client"
    )
    
    # éªŒè¯Agentæ˜¯MCPMixinçš„å®ä¾‹
    print(f"Agentæ˜¯MCPMixinå®ä¾‹: {isinstance(agent1, MCPMixin)}")
    
    # æµ‹è¯•AgentçŠ¶æ€
    status = agent1.get_status()
    print(f"Agent1 çŠ¶æ€: {status}")
    
    # æµ‹è¯•æ¶ˆæ¯å¤„ç†ï¼ˆä¼šè§¦å‘handle_responseï¼‰
    try:
        print("\næµ‹è¯•æ¶ˆæ¯å¤„ç†...")
        response = await agent1.process_message("ä½ å¥½ï¼Œä½ èƒ½å¸®æˆ‘åšä»€ä¹ˆï¼Ÿ")
        print(f"å“åº”: {response.content}")
        print("âœ“ handle_responseå·²è‡ªåŠ¨è°ƒç”¨")
    except Exception as e:
        print(f"æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
    
    # æµ‹è¯•æµå¼å¤„ç†ï¼ˆä¼šè§¦å‘handle_responseï¼‰
    try:
        print("\næµ‹è¯•æµå¼å¤„ç†...")
        async for chunk in agent1.stream_process_message("å‘Šè¯‰æˆ‘ä½ çš„èƒ½åŠ›"):
            print(f"æµå¼å—: {chunk.content}")
        print("âœ“ handle_responseå·²è‡ªåŠ¨è°ƒç”¨")
    except Exception as e:
        print(f"æµå¼å¤„ç†å¤±è´¥: {e}")


async def example_mcp_mixin_functionality():
    """ç¤ºä¾‹ï¼šMCPMixinåŠŸèƒ½æ¼”ç¤º"""
    print("\n=== ç¤ºä¾‹ï¼šMCPMixinåŠŸèƒ½æ¼”ç¤º ===")
    
    # åˆ›å»ºMCPå·¥å…·é…ç½®
    mcp_tools = [
        StdioServerParameters(
            command="echo",
            args=["Hello from MCP mixin"],
            env={}
        )
    ]
    
    # è®¾ç½®LLMå®¢æˆ·ç«¯
    llm_manager = get_llm_client_manager()
    mock_client = MockLLMClient()
    llm_manager.register_client("mixin_client", mock_client)
    
    # åˆ›å»ºAgent
    config = AgentConfig(
        name="MCP Mixinç¤ºä¾‹",
        model="gpt-3.5-turbo",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå±•ç¤ºMCP mixinåŠŸèƒ½çš„AIåŠ©æ‰‹ã€‚",
        mcp_tools=mcp_tools,
        llm_client_name="mixin_client"
    )
    agent = SimpleAgent(config)
    
    # æµ‹è¯•MCPçŠ¶æ€
    mcp_status = agent.get_mcp_status()
    print(f"MCPçŠ¶æ€: {mcp_status}")
    
    # æµ‹è¯•MCPå·¥å…·åˆå§‹åŒ–
    print("\næ‰‹åŠ¨åˆå§‹åŒ–MCPå·¥å…·...")
    try:
        await agent._ensure_mcp_initialized()
        print("âœ“ MCPå·¥å…·åˆå§‹åŒ–æˆåŠŸ")
        print(f"å·¥å…·æ•°é‡: {len(agent.mcp_tools)}")
        print(f"å·¥å…·åŒ…æ•°é‡: {len(agent.mcp_toolkits)}")
    except Exception as e:
        print(f"MCPå·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")


async def example_handle_response_mechanism():
    """ç¤ºä¾‹ï¼šhandle_responseæœºåˆ¶æ¼”ç¤º"""
    print("\n=== ç¤ºä¾‹ï¼šhandle_responseæœºåˆ¶æ¼”ç¤º ===")
    
    # åˆ›å»ºè‡ªå®šä¹‰Agentç±»æ¥å±•ç¤ºhandle_responseæœºåˆ¶
    class CustomAgent(SimpleAgent):
        def __init__(self, config):
            super().__init__(config)
            self.response_count = 0
            self.handled_responses = []
        
        async def handle_response(self, response, **context):
            """é‡å†™handle_responseä»¥å±•ç¤ºæœºåˆ¶"""
            self.response_count += 1
            self.handled_responses.append({
                "content": response.content,
                "context": context,
                "timestamp": "now"
            })
            print(f"ğŸ¯ CustomAgent.handle_responseè¢«è°ƒç”¨ (ç¬¬{self.response_count}æ¬¡)")
            print(f"   å“åº”å†…å®¹: {response.content[:50]}...")
            print(f"   ä¸Šä¸‹æ–‡: {list(context.keys())}")
            
            # è°ƒç”¨çˆ¶ç±»çš„handle_response
            await super().handle_response(response, **context)
    
    # è®¾ç½®LLMå®¢æˆ·ç«¯
    llm_manager = get_llm_client_manager()
    mock_client = MockLLMClient()
    llm_manager.register_client("custom_client", mock_client)
    
    # åˆ›å»ºè‡ªå®šä¹‰Agent
    config = AgentConfig(
        name="è‡ªå®šä¹‰å“åº”å¤„ç†Agent",
        model="gpt-3.5-turbo",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå±•ç¤ºå“åº”å¤„ç†æœºåˆ¶çš„AIåŠ©æ‰‹ã€‚",
        llm_client_name="custom_client"
    )
    agent = CustomAgent(config)
    
    # æµ‹è¯•å“åº”å¤„ç†
    print("\nå‘é€æ¶ˆæ¯...")
    response = await agent.process_message("å±•ç¤ºhandle_responseæœºåˆ¶")
    
    print(f"\nå“åº”å¤„ç†ç»Ÿè®¡:")
    print(f"  å“åº”å¤„ç†æ¬¡æ•°: {agent.response_count}")
    print(f"  å¤„ç†çš„å“åº”æ•°é‡: {len(agent.handled_responses)}")
    
    # æµ‹è¯•æµå¼å¤„ç†çš„å“åº”å¤„ç†
    print("\næµ‹è¯•æµå¼å¤„ç†çš„å“åº”å¤„ç†...")
    async for chunk in agent.stream_process_message("æµå¼å“åº”å¤„ç†"):
        pass  # åªæ˜¯ä¸ºäº†è§¦å‘handle_response
    
    print(f"\næµå¼å¤„ç†åçš„ç»Ÿè®¡:")
    print(f"  å“åº”å¤„ç†æ¬¡æ•°: {agent.response_count}")


async def example_component_handle_response():
    """ç¤ºä¾‹ï¼šç»„ä»¶handle_responseè°ƒç”¨"""
    print("\n=== ç¤ºä¾‹ï¼šç»„ä»¶handle_responseè°ƒç”¨ ===")
    
    # åˆ›å»ºå¸¦æœ‰æ¨¡æ‹Ÿç»„ä»¶çš„Agent
    class MockComponent:
        def __init__(self, name):
            self.name = name
            self.handled_responses = []
        
        async def handle_response(self, response, **context):
            self.handled_responses.append({
                "content": response.content,
                "context": context
            })
            print(f"ğŸ“¦ {self.name}.handle_responseè¢«è°ƒç”¨")
    
    class ComponentAwareAgent(SimpleAgent):
        def __init__(self, config):
            super().__init__(config)
            # æ·»åŠ æ¨¡æ‹Ÿç»„ä»¶
            self.mock_context_engine = MockComponent("MockContextEngine")
            self.mock_llm_client = MockComponent("MockLLMClient")
        
        async def handle_response(self, response, **context):
            """æ¼”ç¤ºç»„ä»¶handle_responseè°ƒç”¨"""
            print(f"ğŸš€ å¼€å§‹è°ƒç”¨ç»„ä»¶handle_responseæ–¹æ³•...")
            
            # è°ƒç”¨æ¨¡æ‹Ÿç»„ä»¶çš„handle_response
            await self.mock_context_engine.handle_response(response, **context)
            await self.mock_llm_client.handle_response(response, **context)
            
            # è°ƒç”¨çˆ¶ç±»çš„handle_response
            await super().handle_response(response, **context)
            
            print(f"âœ… æ‰€æœ‰ç»„ä»¶handle_responseè°ƒç”¨å®Œæˆ")
    
    # è®¾ç½®LLMå®¢æˆ·ç«¯
    llm_manager = get_llm_client_manager()
    mock_client = MockLLMClient()
    llm_manager.register_client("component_client", mock_client)
    
    # åˆ›å»ºAgent
    config = AgentConfig(
        name="ç»„ä»¶æ„ŸçŸ¥Agent",
        model="gpt-3.5-turbo",
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå±•ç¤ºç»„ä»¶å“åº”å¤„ç†çš„AIåŠ©æ‰‹ã€‚",
        llm_client_name="component_client"
    )
    agent = ComponentAwareAgent(config)
    
    # æµ‹è¯•ç»„ä»¶å“åº”å¤„ç†
    print("\nå‘é€æ¶ˆæ¯ä»¥è§¦å‘ç»„ä»¶å“åº”å¤„ç†...")
    response = await agent.process_message("æµ‹è¯•ç»„ä»¶å“åº”å¤„ç†")
    
    print(f"\nç»„ä»¶å“åº”å¤„ç†ç»Ÿè®¡:")
    print(f"  MockContextEngineå¤„ç†æ¬¡æ•°: {len(agent.mock_context_engine.handled_responses)}")
    print(f"  MockLLMClientå¤„ç†æ¬¡æ•°: {len(agent.mock_llm_client.handled_responses)}")


async def example_inheritance_hierarchy():
    """ç¤ºä¾‹ï¼šç»§æ‰¿å±‚æ¬¡ç»“æ„æ¼”ç¤º"""
    print("\n=== ç¤ºä¾‹ï¼šç»§æ‰¿å±‚æ¬¡ç»“æ„ ===")
    
    # è®¾ç½®LLMå®¢æˆ·ç«¯
    llm_manager = get_llm_client_manager()
    mock_client = MockLLMClient()
    llm_manager.register_client("hierarchy_client", mock_client)
    
    # åˆ›å»ºAgent
    config = AgentConfig(
        name="ç»§æ‰¿å±‚æ¬¡ç¤ºä¾‹",
        model="gpt-3.5-turbo",
        mcp_tools=[
            StdioServerParameters(
                command="echo",
                args=["Inheritance demo"],
                env={}
            )
        ],
        llm_client_name="hierarchy_client"
    )
    agent = SimpleAgent(config)
    
    # å±•ç¤ºç»§æ‰¿å±‚æ¬¡
    print(f"Agentç±»: {agent.__class__.__name__}")
    print(f"ç»§æ‰¿è‡ª: {[cls.__name__ for cls in agent.__class__.__bases__]}")
    print(f"æ–¹æ³•è§£æé¡ºåº: {[cls.__name__ for cls in agent.__class__.__mro__]}")
    
    # æµ‹è¯•å„ç§åŠŸèƒ½
    print(f"\nåŠŸèƒ½æµ‹è¯•:")
    print(f"  æ˜¯å¦ä¸ºAgentBaseå®ä¾‹: {hasattr(agent, 'process_message')}")
    print(f"  æ˜¯å¦ä¸ºMCPMixinå®ä¾‹: {hasattr(agent, 'mcp_tools')}")
    print(f"  æ˜¯å¦æœ‰handle_response: {hasattr(agent, 'handle_response')}")
    print(f"  æ˜¯å¦æœ‰MCPåˆå§‹åŒ–: {hasattr(agent, '_ensure_mcp_initialized')}")


async def main():
    """ä¸»å‡½æ•°"""
    print("MCPé›†æˆä¸handle_responseæœºåˆ¶æ¼”ç¤º")
    print("=" * 60)
    
    # è®¾ç½®æ—¥å¿—
    observability = get_observability_manager()
    observability.logger.info("å¼€å§‹MCPé›†æˆä¸å“åº”å¤„ç†ç¤ºä¾‹")
    
    # è¿è¡Œç¤ºä¾‹
    await example_simple_agent_with_mcp()
    await example_mcp_mixin_functionality()
    await example_handle_response_mechanism()
    await example_component_handle_response()
    await example_inheritance_hierarchy()
    
    print("\n" + "=" * 60)
    print("æ‰€æœ‰ç¤ºä¾‹å®Œæˆ")


if __name__ == "__main__":
    asyncio.run(main()) 