# MemRecallAgent å®ç°æ–‡æ¡£

## æ–‡æ¡£ä¿¡æ¯
- **ç‰ˆæœ¬**: 1.0
- **æ—¥æœŸ**: 2026-02-06
- **çŠ¶æ€**: è‰æ¡ˆ

---

## 1. æ¦‚è¿°

MemRecallAgent æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºè®°å¿†æœç´¢å’Œå¬å›çš„ä¸“ç”¨ Agentã€‚å®ƒåŸºäº CodeAgent çš„æ¶æ„è¿›è¡Œäº†ç®€åŒ–å’Œç‰¹åŒ–ï¼Œä¸“æ³¨äºå•ä¸€èŒè´£ï¼šé«˜æ•ˆåœ°æœç´¢å’Œå¬å›ç”¨æˆ·çš„å†å²è®°å¿†ã€‚

### 1.1 è®¾è®¡åŸåˆ™

1. **å•ä¸€èŒè´£**: åªè´Ÿè´£è®°å¿†æœç´¢å’Œå¬å›ï¼Œä¸åšå…¶ä»–å¤„ç†
2. **å·¥å…·å›ºå®š**: å†…ç½®å›ºå®šçš„ 4 ä¸ªå·¥å…·ï¼Œä¸é€šè¿‡ workbench åŠ¨æ€åŠ è½½
3. **å¿«é€Ÿç»ˆæ­¢**: è°ƒç”¨ handoff å·¥å…·åç«‹å³ç»“æŸï¼Œä¸è¿›è¡Œé¢å¤–è¿­ä»£
4. **æ— çŠ¶æ€è®¾è®¡**: æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¿ç•™è¿è¡ŒçŠ¶æ€

### 1.2 ä¸ CodeAgent çš„ä¸»è¦åŒºåˆ«

| ç‰¹æ€§ | CodeAgent | MemRecallAgent |
|------|-----------|----------------|
| Workbench | æ”¯æŒåŠ¨æ€å·¥å…·åŠ è½½ | æ—  workbenchï¼Œå·¥å…·å›ºå®š |
| å·¥å…·æ•°é‡ | åŠ¨æ€ | å›ºå®š 4 ä¸ª |
| è¿­ä»£æ¬¡æ•° | å¯é…ç½® max_tool_iterations | å•æ¬¡æ‰§è¡Œï¼Œhandoff å³ç»“æŸ |
| ä»£ç æ‰§è¡Œ | æ”¯æŒ Python ä»£ç æ‰§è¡Œ | ä¸æ‰§è¡Œä»£ç  |
| èŒè´£èŒƒå›´ | é€šç”¨ä»£ç æ‰§è¡Œ | ä¸“ç”¨è®°å¿†å¬å› |

---

## 2. ç±»å®šä¹‰

### 2.1 MemRecallAgent

```python
from typing import AsyncGenerator, List, Sequence, Optional, Dict, Any, Tuple
import asyncio
import uuid
import json

from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.base import Response, TaskResult
from autogen_agentchat.messages import (
    BaseAgentEvent,
    BaseChatMessage,
    TextMessage,
    ToolCallRequestEvent,
    ToolCallExecutionEvent,
    HandoffMessage,
)
from autogen_core.models import (
    AssistantMessage,
    ChatCompletionClient,
    CreateResult,
    FunctionExecutionResult,
    FunctionExecutionResultMessage,
    LLMMessage,
    SystemMessage,
    UserMessage,
)
from autogen_agentchat.messages import ModelClientStreamingChunkEvent, ThoughtEvent
from autogen_core import CancellationToken, FunctionCall
from autogen_core.model_context import ChatCompletionContext, UnboundedChatCompletionContext
from autogen_agentchat.utils import remove_images
from pydantic import BaseModel, Field
import logging

from base.groupchat_queue import BaseChatQueue
from base.handoff import ToolType
from data_layer.data_layer import AgentFusionDataLayer

# å¯¼å…¥å·¥å…·å‡½æ•°ï¼ˆä» memrecall_agent_tools.md å®šä¹‰ï¼‰
from tools.memrecall_tools import (
    search_memories_tool,
    get_memory_detail_tool,
    extract_search_keywords_tool,
    expand_context_window_tool,
    handoff_tool,
    SearchMemoriesInput,
    GetMemoryDetailInput,
    ExtractKeywordsInput,
    ExpandContextWindowInput,
    HandoffInput,
    MEMRECALL_TOOLS,
)

logger = logging.getLogger(__name__)


class MemRecallAgent(BaseChatQueue, BaseChatAgent):
    """
    ä¸“é—¨ç”¨äºè®°å¿†å¬å›çš„ Agentã€‚

    MemRecallAgent æ¥æ”¶ç”¨æˆ·çš„è®°å¿†æŸ¥è¯¢è¯·æ±‚ï¼Œä½¿ç”¨å†…ç½®å·¥å…·æœç´¢ç›¸å…³è®°å¿†ï¼Œ
    ç„¶åé€šè¿‡ handoff å°†ç»“æœè¿”å›ç»™çˆ¶ Agentã€‚

    ç‰¹ç‚¹:
    - å·¥å…·å›ºå®šï¼ˆ4 ä¸ªå†…ç½®å·¥å…·ï¼‰
    - è°ƒç”¨ handoff åç«‹å³ç»“æŸ
    - æ”¯æŒæµå¼è¾“å‡ºï¼ˆæ€è€ƒè¿‡ç¨‹ï¼‰
    """

    # ç³»ç»Ÿæç¤ºè¯åŸºç¡€æ¨¡æ¿ï¼ˆåŠ¨æ€ç”Ÿæˆæ—¶ä¼šæ·»åŠ è¿­ä»£çŠ¶æ€ï¼‰
    DEFAULT_SYSTEM_MESSAGE_TEMPLATE = """ä½ æ˜¯ä¸€ä¸ªä¸“é—¨è´Ÿè´£æœç´¢å’Œå¬å›ç”¨æˆ·å†å²è®°å¿†çš„åŠ©æ‰‹ã€‚

## ä½ çš„èŒè´£
1. åˆ†æç”¨æˆ·çš„æŸ¥è¯¢æ„å›¾ï¼Œç†è§£ä»–ä»¬æƒ³è¦æ‰¾ä»€ä¹ˆå†å²è®°å¿†
2. ä½¿ç”¨ search_memories å·¥å…·æœç´¢ç›¸å…³è®°å¿†
3. å¦‚éœ€è¦ï¼Œä½¿ç”¨ get_memory_detail è·å–å®Œæ•´å†…å®¹
4. æ•´ç†æœç´¢ç»“æœï¼Œé€šè¿‡ handoff å·¥å…·ç»“æŸä»»åŠ¡å¹¶è¿”å›ç»“æœ

## å·¥å…·ä½¿ç”¨æŒ‡å—

### 1. search_memoriesï¼ˆä¸»è¦å·¥å…·ï¼‰
- ç”¨äºæœç´¢ç”¨æˆ·çš„å†å²è®°å¿†
- ä¼˜å…ˆä½¿ç”¨ hybrid æ¨¡å¼ï¼Œå®ƒç»“åˆäº†è¯­ä¹‰å’Œå…³é”®è¯åŒ¹é…
- å¦‚æœç”¨æˆ·æåˆ°å…·ä½“æ—¶é—´ï¼ˆå¦‚"ä¸Šå‘¨"ã€"æ˜¨å¤©"ï¼‰ï¼Œä½¿ç”¨ time_range_days å‚æ•°
- å¦‚æœç”¨æˆ·æåˆ°å…·ä½“ç±»å‹ï¼ˆå¦‚"é…ç½®"ã€"å‘½ä»¤"ï¼‰ï¼Œä½¿ç”¨ memory_types å‚æ•°

### 2. get_memory_detailï¼ˆè¾…åŠ©å·¥å…·ï¼‰
- å½“ search_memories è¿”å›çš„æ‘˜è¦ä¸å¤Ÿè¯¦ç»†æ—¶ä½¿ç”¨
- éœ€è¦ memory_keyï¼ˆä» search_memories ç»“æœä¸­è·å–ï¼‰

### 3. extract_search_keywordsï¼ˆå¯é€‰å·¥å…·ï¼‰
- å½“ç”¨æˆ·æŸ¥è¯¢å¾ˆå¤æ‚ï¼Œä¸ç¡®å®šæœç´¢ä»€ä¹ˆå…³é”®è¯æ—¶ä½¿ç”¨
- é€šå¸¸ search_memories ä¼šè‡ªåŠ¨å¤„ç†ï¼Œå¾ˆå°‘éœ€è¦ç›´æ¥è°ƒç”¨

### 4. expand_context_windowï¼ˆæ‹“å±•å·¥å…·ï¼‰
- **å½“å½“å‰æœç´¢ç»“æœä¸ç†æƒ³ï¼Œéœ€è¦æŸ¥çœ‹æ›´å¤šå†å²æ¶ˆæ¯æ—¶ä½¿ç”¨**
- è°ƒç”¨åä¼šç«‹å³ç»“æŸå½“å‰è¿­ä»£ï¼Œç³»ç»Ÿå°†æä¾›æ›´å¤šæ¶ˆæ¯é‡æ–°å‘èµ·è°ƒç”¨
- åªèƒ½åœ¨æœªè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°æ—¶ä½¿ç”¨

### 5. handoffï¼ˆå¿…é¡»æœ€ç»ˆè°ƒç”¨ï¼‰
- **é‡è¦ï¼šå®Œæˆæœç´¢åå¿…é¡»è°ƒç”¨æ­¤å·¥å…·ï¼**
- ä¼ å…¥æ‰€æœ‰ç›¸å…³è®°å¿†çš„æ€»ç»“
- åŒ…å«ä½ çš„ç›¸å…³æ€§åˆ†æå’Œç½®ä¿¡åº¦
- å¦‚æœæ²¡æ‰¾åˆ°ç›¸å…³è®°å¿†ï¼Œä¹Ÿè¦è°ƒç”¨å¹¶è¯´æ˜æƒ…å†µ

## è¿­ä»£å·¥ä½œæµç¨‹

1. **åˆ†æ**: åŸºäºå½“å‰å¯ç”¨çš„æ¶ˆæ¯åˆ†æç”¨æˆ·æ„å›¾
2. **æœç´¢**: è°ƒç”¨ search_memories æœç´¢è®°å¿†ï¼ˆå¯è°ƒæ•´å‚æ•°å¤šæ¬¡æœç´¢ï¼‰
3. **è¯„ä¼°**: è¯„ä¼°æœç´¢ç»“æœè´¨é‡
4. **å†³ç­–**:
   - ç»“æœæ»¡æ„ â†’ è°ƒç”¨ handoff ç»“æŸ
   - éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ â†’ è°ƒç”¨ expand_context_windowï¼ˆå¦‚è¿˜æœ‰è¿­ä»£æ¬¡æ•°ï¼‰
   - è¾¾åˆ°æœ€å¤§è¿­ä»£ â†’ è°ƒç”¨ handoff ç»“æŸï¼ˆæŠ¥å‘Šå½“å‰æœ€ä½³ç»“æœï¼‰

## æ³¨æ„äº‹é¡¹

- ä¸è¦å‘ç”¨æˆ·ç›´æ¥å›å¤ï¼Œä½ çš„ç»“æœåº”è¯¥é€šè¿‡ handoff å·¥å…·è¿”å›
- å¦‚æœæœç´¢ç»“æœä¸ç†æƒ³ï¼Œä¼˜å…ˆè€ƒè™‘æ‹“å±•ä¸Šä¸‹æ–‡çª—å£ï¼ˆå¦‚æœè¿˜æœ‰è¿­ä»£æ¬¡æ•°ï¼‰
- ç½®ä¿¡åº¦ä½äº 0.5 æ—¶ï¼Œè€ƒè™‘è®¾ç½® needs_more_info=True
- å§‹ç»ˆä¿æŒä¸“ä¸šã€å‡†ç¡®çš„æœç´¢æ€åº¦
"""

    def __init__(
        self,
        name: str,
        model_client: ChatCompletionClient,
        data_layer: AgentFusionDataLayer,
        user_id: int,
        model_context: Optional[ChatCompletionContext] = None,
        system_message: Optional[str] = None,
        max_search_iterations: int = 3,  # æœ€å¤šæœç´¢æ¬¡æ•°
    ):
        """
        åˆå§‹åŒ– MemRecallAgent

        Args:
            name: Agent åç§°
            model_client: LLM å®¢æˆ·ç«¯
            data_layer: æ•°æ®å±‚è®¿é—®æ¥å£
            user_id: å½“å‰ç”¨æˆ· IDï¼ˆç”¨äºæ•°æ®éš”ç¦»ï¼‰
            model_context: å¯é€‰çš„æ¨¡å‹ä¸Šä¸‹æ–‡
            system_message: å¯é€‰çš„è‡ªå®šä¹‰ç³»ç»Ÿæç¤º
            max_search_iterations: æœ€å¤šæœç´¢è¿­ä»£æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
        """
        BaseChatAgent.__init__(
            self,
            name,
            "A specialized agent for searching and recalling user memories."
        )
        BaseChatQueue.__init__(self)

        self._model_client = model_client
        self._data_layer = data_layer
        self._user_id = user_id
        self._system_message_template = system_message or self.DEFAULT_SYSTEM_MESSAGE_TEMPLATE
        self._max_iterations = max_search_iterations

        # è¿­ä»£çŠ¶æ€è·Ÿè¸ª
        self._current_iteration = 1
        self._context_window_size = 5  # é»˜è®¤ä½¿ç”¨æœ€è¿‘5æ¡æ¶ˆæ¯

        # åˆå§‹åŒ–æ¨¡å‹ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨åŠ¨æ€ç”Ÿæˆçš„ç³»ç»Ÿæç¤ºè¯ï¼‰
        if model_context is None:
            initial_prompt = self._build_system_prompt(iteration=1, max_iterations=max_search_iterations)
            model_context = UnboundedChatCompletionContext([
                SystemMessage(content=initial_prompt, source="system")
            ])
        self._model_context = model_context

        # å†…éƒ¨çŠ¶æ€
        self._is_running = False
        self._cancellation_token: Optional[CancellationToken] = None
        self._search_count = 0  # è®°å½•æœç´¢æ¬¡æ•°

        # é¢„å®šä¹‰å·¥å…·åˆ—è¡¨ï¼ˆå›ºå®šå·¥å…·ï¼Œæ—  workbenchï¼‰
        self._tools = self._build_tool_schemas()
        self._handoff_tool_name = "handoff"

    def _build_tool_schemas(self) -> List[Dict[str, Any]]:
        """æ„å»ºå·¥å…· schema åˆ—è¡¨ï¼ˆå›ºå®šå·¥å…·ï¼‰"""
        tools = []
        for tool_name, tool_info in MEMRECALL_TOOLS.items():
            tool_schema = {
                "name": tool_info["name"],
                "description": tool_info["description"],
                "parameters": tool_info["input_model"].model_json_schema(),
                "type": ToolType.HANDOFF_TOOL if tool_info.get("is_handoff") else ToolType.NORMAL_TOOL,
            }
            tools.append(tool_schema)
        return tools

    def _build_system_prompt(self, iteration: int, max_iterations: int) -> str:
        """
        æ„å»ºåŒ…å«è¿­ä»£çŠ¶æ€çš„åŠ¨æ€ç³»ç»Ÿæç¤ºè¯

        Args:
            iteration: å½“å‰è¿­ä»£è½®æ•°ï¼ˆä»1å¼€å§‹ï¼‰
            max_iterations: æœ€å¤§å…è®¸è¿­ä»£æ¬¡æ•°

        Returns:
            å®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯ï¼ŒåŒ…å«ç¯å¢ƒæ„ŸçŸ¥ä¿¡æ¯
        """
        base_prompt = self._system_message_template

        # æ·»åŠ ç¯å¢ƒçŠ¶æ€ä¿¡æ¯
        environment_status = f"""

## å½“å‰ç¯å¢ƒçŠ¶æ€ï¼ˆENVIRONMENT STATUSï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”„ å½“å‰è¿­ä»£è½®æ•°: {iteration}/{max_iterations}
ğŸ“Š ä¸Šä¸‹æ–‡çª—å£: æœ€è¿‘ {self._context_window_size} æ¡æ¶ˆæ¯å¯ç”¨
ğŸ“¨ æ€»æ¶ˆæ¯æ•°: æ ¹æ®ä¼ å…¥çš„æ¶ˆæ¯åˆ—è¡¨åŠ¨æ€ç¡®å®š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""

        # å¦‚æœæ˜¯åç»­è¿­ä»£ï¼Œæ·»åŠ è­¦å‘Šæç¤º
        if iteration > 1:
            environment_status += f"""
âš ï¸  **è¿™æ˜¯ç¬¬ {iteration} è½®è¿­ä»£**
ä¹‹å‰çš„æœç´¢ç»“æœæœªèƒ½æ‰¾åˆ°æ»¡æ„çš„è®°å¿†ï¼Œå·²æ‹“å±•ä¸Šä¸‹æ–‡çª—å£ã€‚
ç°åœ¨æœ‰æ›´å¤šå†å²æ¶ˆæ¯å¯ä¾›åˆ†æï¼Œè¯·é‡æ–°è¯„ä¼°å¹¶æœç´¢ã€‚
"""

        # æ·»åŠ è¿­ä»£é™åˆ¶æç¤º
        if iteration >= max_iterations:
            environment_status += f"""
ğŸš« **æ³¨æ„ï¼šå·²è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({max_iterations})**
æœ¬è½®æœç´¢åå¿…é¡»è°ƒç”¨ handoff ç»“æŸä»»åŠ¡ï¼Œæ— è®ºç»“æœå¦‚ä½•ã€‚
"""
        else:
            remaining = max_iterations - iteration
            environment_status += f"""
ğŸ’¡ æç¤ºï¼šè¿˜å¯æ‹“å±• {remaining} æ¬¡ä¸Šä¸‹æ–‡çª—å£ï¼ˆå¦‚éœ€ï¼‰
"""

        return base_prompt + environment_status

    def set_iteration_state(self, iteration: int, context_window_size: int) -> None:
        """
        è®¾ç½®å½“å‰è¿­ä»£çŠ¶æ€ï¼ˆç”± MemoryContext åœ¨é‡æ–°å‘èµ·è°ƒç”¨æ—¶è®¾ç½®ï¼‰

        Args:
            iteration: å½“å‰è¿­ä»£è½®æ•°
            context_window_size: å½“å‰ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆæ¶ˆæ¯æ•°ï¼‰
        """
        self._current_iteration = iteration
        self._context_window_size = context_window_size

        # æ›´æ–°æ¨¡å‹ä¸Šä¸‹æ–‡ä¸­çš„ç³»ç»Ÿæç¤ºè¯
        new_prompt = self._build_system_prompt(iteration, self._max_iterations)
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ¸…ç©ºä¸Šä¸‹æ–‡å¹¶é‡æ–°æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        # å®é™…å®ç°å¯èƒ½éœ€è¦æ ¹æ®å…·ä½“ä¸Šä¸‹æ–‡ç±»å‹è°ƒæ•´

    @property
    def produced_message_types(self) -> Sequence[type[BaseChatMessage]]:
        """æ­¤ Agent å¯ä»¥äº§ç”Ÿçš„æ¶ˆæ¯ç±»å‹"""
        return [TextMessage, ToolCallRequestEvent, ToolCallExecutionEvent, HandoffMessage]

    async def start(
        self,
        cancellation_token: Optional[CancellationToken] = None,
        output_task_messages: bool = True
    ) -> None:
        """å¯åŠ¨ Agent"""
        if self._is_running:
            raise ValueError("Agent is already running")

        self._cancellation_token = cancellation_token
        self._is_running = True
        self._search_count = 0

    async def push(
        self,
        messages: Union[str, List[LLMMessage]]
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | TaskResult, None]:
        """
        Push æ¥å£æ¥æ”¶æ–°æ¶ˆæ¯

        æ”¯æŒä¸¤ç§è¾“å…¥æ ¼å¼:
        - str: ç”¨æˆ·æŸ¥è¯¢å­—ç¬¦ä¸²
        - List[LLMMessage]: å®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
        """
        try:
            # è½¬æ¢è¾“å…¥ä¸ºæ¶ˆæ¯åˆ—è¡¨
            if isinstance(messages, str):
                # æ„å»ºåŒ…å«ç”¨æˆ·æŸ¥è¯¢çš„æ¶ˆæ¯
                user_message = TextMessage(content=messages, source="user")
                messages_to_process = [user_message]
            else:
                messages_to_process = messages

            # è°ƒç”¨æµå¼å¤„ç†
            async for result in self.on_messages_stream(
                messages_to_process,
                self._cancellation_token
            ):
                # åˆ†å‘æ¶ˆæ¯åˆ°å¯¹åº”å¤„ç†å™¨
                await self._dispatch_message(result)

        except Exception as e:
            raise RuntimeError(f"Error in MemRecallAgent push: {str(e)}") from e

    async def _dispatch_message(
        self,
        message: BaseAgentEvent | BaseChatMessage | TaskResult | Response
    ) -> None:
        """æ ¹æ®æ¶ˆæ¯ç±»å‹åˆ†å‘åˆ°å¯¹åº”å¤„ç†å™¨"""
        if isinstance(message, TaskResult):
            await self.handle_task_result(message)
        elif isinstance(message, Response):
            await self.handle_response(message)
        elif isinstance(message, ModelClientStreamingChunkEvent):
            await self.handle_streaming_chunk(message)
        elif isinstance(message, ThoughtEvent):
            await self.handle_thought(message)
        elif isinstance(message, BaseAgentEvent):
            await self.handle_agent_event(message)
        elif isinstance(message, BaseChatMessage):
            await self.handle_chat_message(message)
        else:
            await self.handle_unknown_message(message)

    # --- æ¶ˆæ¯å¤„ç†å™¨ï¼ˆå¯è¢«é‡å†™ï¼‰ ---

    async def handle_task_result(self, message: TaskResult) -> None:
        """å¤„ç† TaskResult"""
        pass

    async def handle_response(self, message: Response) -> None:
        """å¤„ç† Response"""
        pass

    async def handle_agent_event(self, message: BaseAgentEvent) -> None:
        """å¤„ç† Agent äº‹ä»¶"""
        pass

    async def handle_chat_message(self, message: BaseChatMessage) -> None:
        """å¤„ç†èŠå¤©æ¶ˆæ¯"""
        pass

    async def handle_streaming_chunk(self, message: ModelClientStreamingChunkEvent) -> None:
        """å¤„ç†æµå¼è¾“å‡ºå—"""
        pass

    async def handle_thought(self, message: ThoughtEvent) -> None:
        """å¤„ç†æ€è€ƒäº‹ä»¶"""
        pass

    async def handle_unknown_message(self, message: Any) -> None:
        """å¤„ç†æœªçŸ¥æ¶ˆæ¯ç±»å‹"""
        logger.warning(f"Unknown message type in MemRecallAgent: {type(message)}")

    async def task_finished(self, task_result: TaskResult) -> None:
        """ä»»åŠ¡å®Œæˆå¤„ç†"""
        self._is_running = False

    # --- æ ¸å¿ƒå¤„ç†é€»è¾‘ ---

    async def on_messages_stream(
        self,
        messages: Sequence[BaseChatMessage],
        cancellation_token: Optional[CancellationToken]
    ) -> AsyncGenerator[BaseAgentEvent | BaseChatMessage | Response, None]:
        """
        æµå¼æ¶ˆæ¯å¤„ç†æ ¸å¿ƒé€»è¾‘

        æµç¨‹:
        1. æ·»åŠ æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
        2. è°ƒç”¨ LLM è·å–å·¥å…·è°ƒç”¨
        3. æ‰§è¡Œå·¥å…·è°ƒç”¨
        4. å¦‚æœæ˜¯ handoff å·¥å…·ï¼Œç«‹å³ç»“æŸ
        5. å¦åˆ™ç»§ç»­è¿­ä»£ï¼ˆæœ€å¤š max_search_iterations æ¬¡ï¼‰
        """
        message_id = str(uuid.uuid4())

        # æ·»åŠ æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
        for message in messages:
            await self._model_context.add_message(message.to_model_message())

        # è·å–å·¥å…· schemasï¼ˆä¼ é€’ç»™ LLMï¼‰
        tool_schemas = self._get_tool_schemas_for_llm()

        # è¿­ä»£å¤„ç†å·¥å…·è°ƒç”¨
        for iteration in range(self._max_search_iterations):
            # è°ƒç”¨ LLM
            llm_messages = await self._get_compatible_context()

            model_result: Optional[CreateResult] = None
            async for chunk in self._call_llm(
                message_id,
                llm_messages,
                tool_schemas,
                cancellation_token
            ):
                if isinstance(chunk, CreateResult):
                    model_result = chunk
                elif isinstance(chunk, ModelClientStreamingChunkEvent):
                    yield chunk

            if model_result is None:
                raise RuntimeError("No model result produced")

            # è¾“å‡ºæ€è€ƒå†…å®¹
            if model_result.thought:
                yield ThoughtEvent(content=model_result.thought, source=self.name)

            # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯
            assistant_message = AssistantMessage(
                content=model_result.content,
                source=self.name,
                thought=getattr(model_result, "thought", None),
            )
            await self._model_context.add_message(assistant_message)

            # æ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·è°ƒç”¨
            if isinstance(model_result.content, str):
                # ä¸æ˜¯å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆå“åº”
                yield self._create_response(model_result, message_id)
                return

            # æ˜¯å·¥å…·è°ƒç”¨
            tool_calls = model_result.content
            if not isinstance(tool_calls, list) or not all(
                isinstance(tc, FunctionCall) for tc in tool_calls
            ):
                yield self._create_response(model_result, message_id)
                return

            # å‘é€å·¥å…·è°ƒç”¨è¯·æ±‚äº‹ä»¶
            tool_call_msg = ToolCallRequestEvent(
                content=tool_calls,
                source=self.name,
                models_usage=model_result.usage,
            )
            yield tool_call_msg

            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            exec_results = await self._execute_tool_calls(tool_calls)

            # å‘é€å·¥å…·æ‰§è¡Œç»“æœäº‹ä»¶
            tool_result_msg = ToolCallExecutionEvent(
                content=exec_results,
                source=self.name,
            )
            yield tool_result_msg

            # æ·»åŠ å·¥å…·ç»“æœåˆ°ä¸Šä¸‹æ–‡
            await self._model_context.add_message(
                FunctionExecutionResultMessage(content=exec_results)
            )

            # æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†ç»ˆæ­¢ç±»å·¥å…·ï¼ˆhandoff æˆ– expand_context_windowï¼‰
            is_termination, termination_type = self._check_termination_call(tool_calls, exec_results)

            if is_termination and termination_type == "handoff":
                # è°ƒç”¨äº† handoffï¼Œä»»åŠ¡å®Œæˆ
                handoff_response = self._create_handoff_response(exec_results, model_result)
                if handoff_response:
                    yield handoff_response
                return

            elif is_termination and termination_type == "expand":
                # è°ƒç”¨äº† expand_context_windowï¼Œæ£€æŸ¥æ˜¯å¦è¢«æ‰¹å‡†
                if self._is_expand_approved(exec_results):
                    # æ‹“å±•è¯·æ±‚å·²æ‰¹å‡†ï¼Œç»“æŸå½“å‰è¿­ä»£
                    # MemoryContext å°†é‡æ–°å‘èµ·è°ƒç”¨ï¼ˆè¿­ä»£è®¡æ•°+1ï¼‰
                    yield TextMessage(
                        content="[ç³»ç»Ÿ] ä¸Šä¸‹æ–‡æ‹“å±•è¯·æ±‚å·²æ‰¹å‡†ï¼Œå‡†å¤‡é‡æ–°æœç´¢...",
                        source=self.name
                    )
                    return
                else:
                    # æ‹“å±•è¢«æ‹’ç»ï¼ˆè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼‰ï¼Œç»§ç»­æœç´¢æˆ– handoff
                    logger.warning("Expand context window request was denied")
                    # ç»§ç»­ä¸‹ä¸€è½®è¿­ä»£ï¼Œè®© Agent å†³å®šæ˜¯å¦ handoff

            # ä¸æ˜¯ç»ˆæ­¢è°ƒç”¨ï¼Œç»§ç»­ä¸‹ä¸€è½®è¿­ä»£
            self._search_count += 1

        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¼ºåˆ¶ç»“æŸ
        logger.warning(f"Max search iterations ({self._max_search_iterations}) reached")
        yield TextMessage(
            content="[ç³»ç»Ÿ] è¾¾åˆ°æœ€å¤§æœç´¢æ¬¡æ•°é™åˆ¶ï¼Œç»“æŸè®°å¿†å¬å›ã€‚",
            source=self.name
        )

    def _get_tool_schemas_for_llm(self) -> List[Dict[str, Any]]:
        """è·å–ä¼ é€’ç»™ LLM çš„å·¥å…· schemas"""
        return [
            {
                "name": tool["name"],
                "description": tool["description"],
                "parameters": tool["parameters"],
            }
            for tool in self._tools
        ]

    async def _execute_tool_calls(
        self,
        tool_calls: List[FunctionCall]
    ) -> List[FunctionExecutionResult]:
        """æ‰§è¡Œå·¥å…·è°ƒç”¨åˆ—è¡¨"""
        results = []

        for call in tool_calls:
            result = await self._execute_single_tool(call)
            results.append(result)

        return results

    async def _execute_single_tool(
        self,
        tool_call: FunctionCall
    ) -> FunctionExecutionResult:
        """æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨"""
        try:
            arguments = json.loads(tool_call.arguments)
        except json.JSONDecodeError as e:
            return FunctionExecutionResult(
                content=f"Error parsing arguments: {e}",
                call_id=tool_call.id,
                is_error=True,
                name=tool_call.name,
            )

        # æŸ¥æ‰¾å·¥å…·
        tool_info = MEMRECALL_TOOLS.get(tool_call.name)
        if not tool_info:
            return FunctionExecutionResult(
                content=f"Unknown tool: {tool_call.name}",
                call_id=tool_call.id,
                is_error=True,
                name=tool_call.name,
            )

        # æ‰§è¡Œå·¥å…·
        try:
            if tool_call.name == "search_memories":
                input_data = SearchMemoriesInput(**arguments)
                output = await search_memories_tool(
                    self._data_layer,
                    self._user_id,
                    input_data
                )

            elif tool_call.name == "get_memory_detail":
                input_data = GetMemoryDetailInput(**arguments)
                output = await get_memory_detail_tool(
                    self._data_layer,
                    self._user_id,
                    input_data
                )

            elif tool_call.name == "extract_search_keywords":
                input_data = ExtractKeywordsInput(**arguments)
                output = await extract_search_keywords_tool(
                    self._model_client,
                    input_data
                )

            elif tool_call.name == "expand_context_window":
                input_data = ExpandContextWindowInput(**arguments)
                output = await expand_context_window_tool(
                    input_data,
                    current_iteration=self._current_iteration,
                    max_iterations=self._max_iterations
                )

            elif tool_call.name == "handoff":
                input_data = HandoffInput(**arguments)
                output = await handoff_tool(input_data)

            else:
                return FunctionExecutionResult(
                    content=f"Unhandled tool: {tool_call.name}",
                    call_id=tool_call.id,
                    is_error=True,
                    name=tool_call.name,
                )

            return FunctionExecutionResult(
                content=output.model_dump_json(),
                call_id=tool_call.id,
                is_error=not output.success,
                name=tool_call.name,
            )

        except Exception as e:
            logger.error(f"Error executing tool {tool_call.name}: {e}")
            return FunctionExecutionResult(
                content=f"Tool execution error: {str(e)}",
                call_id=tool_call.id,
                is_error=True,
                name=tool_call.name,
            )

    def _check_termination_call(
        self,
        tool_calls: List[FunctionCall],
        exec_results: List[FunctionExecutionResult]
    ) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ˜¯å¦è°ƒç”¨äº†ç»ˆæ­¢ç±»å·¥å…·ï¼ˆhandoff æˆ– expand_context_windowï¼‰

        Returns:
            (is_termination, termination_type)
            - is_termination: æ˜¯å¦è°ƒç”¨äº†ç»ˆæ­¢å·¥å…·
            - termination_type: "handoff" | "expand" | ""
        """
        for call in tool_calls:
            if call.name == "handoff":
                return True, "handoff"
            elif call.name == "expand_context_window":
                return True, "expand"
        return False, ""

    def _is_expand_approved(self, exec_results: List[FunctionExecutionResult]) -> bool:
        """æ£€æŸ¥ expand_context_window æ˜¯å¦è¢«æ‰¹å‡†"""
        for result in exec_results:
            if result.name == "expand_context_window" and not result.is_error:
                try:
                    output = json.loads(result.content)
                    return output.get("approved", False)
                except json.JSONDecodeError:
                    pass
        return False

    def _create_handoff_response(
        self,
        exec_results: List[FunctionExecutionResult],
        model_result: CreateResult
    ) -> Optional[HandoffMessage]:
        """ä»æ‰§è¡Œç»“æœåˆ›å»º HandoffMessage"""
        for result in exec_results:
            if result.name == self._handoff_tool_name and not result.is_error:
                try:
                    output = json.loads(result.content)
                    if output.get("transfer_completed"):
                        return HandoffMessage(
                            content=output.get("message", "è®°å¿†æœç´¢å®Œæˆ"),
                            target="parent",  # å›ºå®šäº¤è¿˜ç»™çˆ¶ Agent
                            source=self.name,
                            context=self._build_handoff_context(model_result)
                        )
                except json.JSONDecodeError:
                    pass
        return None

    def _build_handoff_context(self, model_result: CreateResult) -> List[LLMMessage]:
        """æ„å»ºç§»äº¤ä¸Šä¸‹æ–‡"""
        context: List[LLMMessage] = []

        # æ·»åŠ æ€è€ƒå†…å®¹
        if model_result.thought:
            context.append(AssistantMessage(
                content=model_result.thought,
                source=self.name,
            ))

        return context

    async def _call_llm(
        self,
        message_id: str,
        llm_messages: Sequence[LLMMessage],
        tools: List[Dict[str, Any]],
        cancellation_token: Optional[CancellationToken],
    ) -> AsyncGenerator[Union[ModelClientStreamingChunkEvent, CreateResult], None]:
        """è°ƒç”¨ LLM"""
        try:
            if hasattr(self._model_client, 'create_stream'):
                async for chunk in self._model_client.create_stream(
                    llm_messages,
                    tools=tools,
                    cancellation_token=cancellation_token or CancellationToken(),
                ):
                    if isinstance(chunk, CreateResult):
                        yield chunk
                    elif isinstance(chunk, str):
                        yield ModelClientStreamingChunkEvent(
                            content=chunk,
                            source=self.name,
                            full_message_id=message_id
                        )
                    else:
                        raise RuntimeError(f"Invalid chunk type: {type(chunk)}")
            else:
                # éæµå¼è°ƒç”¨
                result = await self._model_client.create(
                    llm_messages,
                    tools=tools,
                    cancellation_token=cancellation_token or CancellationToken(),
                )
                yield result
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            raise

    async def _get_compatible_context(self) -> Sequence[LLMMessage]:
        """è·å–å…¼å®¹çš„ä¸Šä¸‹æ–‡ï¼ˆç§»é™¤å›¾åƒå¦‚æœæ¨¡å‹ä¸æ”¯æŒï¼‰"""
        messages = await self._model_context.get_messages()
        if self._model_client.model_info.get("vision", False):
            return messages
        return remove_images(messages)

    def _create_response(self, model_result: CreateResult, message_id: str) -> Response:
        """åˆ›å»º Response å¯¹è±¡"""
        return Response(
            chat_message=TextMessage(
                content=model_result.content
                if isinstance(model_result.content, str)
                else "è®°å¿†æœç´¢å®Œæˆ",
                source=self.name
            )
        )

    async def on_messages(
        self,
        messages: Sequence[BaseChatMessage],
        cancellation_token: Optional[CancellationToken]
    ) -> Response:
        """éæµå¼æ¶ˆæ¯å¤„ç†"""
        result_messages = []
        async for chunk in self.on_messages_stream(messages, cancellation_token):
            if isinstance(chunk, Response):
                return chunk
            result_messages.append(chunk)

        # é»˜è®¤å“åº”
        return Response(
            chat_message=TextMessage(
                content="è®°å¿†å¬å›å¤„ç†å®Œæˆ",
                source=self.name
            )
        )

    async def on_reset(self, cancellation_token: Optional[CancellationToken]) -> None:
        """é‡ç½® Agent çŠ¶æ€"""
        if self._model_context:
            await self._model_context.clear()
        self._is_running = False
        self._search_count = 0

    def get_search_count(self) -> int:
        """è·å–æœ¬æ¬¡è¿è¡Œçš„æœç´¢æ¬¡æ•°"""
        return self._search_count
```

---

## 3. ä½¿ç”¨ç¤ºä¾‹

### 3.1 åŸºæœ¬ä½¿ç”¨

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient
from data_layer.data_layer import AgentFusionDataLayer

# åˆå§‹åŒ–
model_client = OpenAIChatCompletionClient(model="gpt-4")
data_layer = AgentFusionDataLayer(database_url="...")

# åˆ›å»º Agent
mem_recall_agent = MemRecallAgent(
    name="memory_searcher",
    model_client=model_client,
    data_layer=data_layer,
    user_id=123,
    max_search_iterations=3
)

# ä½¿ç”¨
await mem_recall_agent.start()

async for event in mem_recall_agent.push("æŸ¥æ‰¾æˆ‘ä¹‹å‰çš„æ•°æ®åº“é…ç½®"):
    if isinstance(event, HandoffMessage):
        print(f"æœç´¢ç»“æœ: {event.content}")
        print(f"ç›¸å…³è®°å¿†: {event.context}")
    elif isinstance(event, ThoughtEvent):
        print(f"æ€è€ƒ: {event.content}")
    elif isinstance(event, TextMessage):
        print(f"æ¶ˆæ¯: {event.content}")
```

### 3.2 åœ¨ Group Chat ä¸­ä½¿ç”¨

```python
from autogen_agentchat.teams import SelectorGroupChat

# åˆ›å»ºè®°å¿†å¬å› Agent
memory_agent = MemRecallAgent(
    name="memory_recall",
    model_client=model_client,
    data_layer=data_layer,
    user_id=user_id
)

# åˆ›å»ºä¸»å¤„ç† Agent
main_agent = AssistantAgent(
    name="main_assistant",
    model_client=model_client,
    system_message="ä½ æ˜¯ä¸»åŠ©æ‰‹ï¼Œéœ€è¦è®°å¿†æ—¶äº¤ç»™ memory_recall å¤„ç†"
)

# é…ç½® Group Chat
team = SelectorGroupChat(
    participants=[main_agent, memory_agent],
    model_client=model_client,
    selector_prompt="""
    æ ¹æ®ç”¨æˆ·è¯·æ±‚é€‰æ‹©åˆé€‚çš„ Agent:
    - å¦‚æœç”¨æˆ·æåˆ°å†å²ä¿¡æ¯ã€ä¹‹å‰çš„é…ç½®ã€ä»¥å‰çš„è¯é¢˜ï¼Œé€‰æ‹© memory_recall
    - å…¶ä»–æƒ…å†µé€‰æ‹© main_assistant
    """
)

# è¿è¡Œ
result = await team.run(task="æŒ‰ç…§æˆ‘ä¹‹å‰çš„æ•°æ®åº“é…ç½®æ¥")
```

### 3.3 è‡ªå®šä¹‰ç³»ç»Ÿæç¤º

```python
custom_system_message = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®°å¿†æœç´¢åŠ©æ‰‹ã€‚

ç‰¹æ®Šè§„åˆ™:
1. å¯¹äºé…ç½®ç±»æŸ¥è¯¢ï¼Œä¼˜å…ˆæœç´¢ memory_types=["config"]
2. å¦‚æœæœç´¢ç»“æœä¸ºç©ºï¼Œå°è¯•ä½¿ç”¨æ›´å®½æ³›çš„å…³é”®è¯
3. ç§»äº¤æ—¶å§‹ç»ˆåŒ…å«å®Œæ•´çš„è®°å¿†å†…å®¹æ‘˜è¦
"""

agent = MemRecallAgent(
    name="custom_memory_agent",
    model_client=model_client,
    data_layer=data_layer,
    user_id=user_id,
    system_message=custom_system_message,
    max_search_iterations=5  # å…è®¸æ›´å¤šæœç´¢å°è¯•
)
```

---

## 4. é…ç½®å‚æ•°

### 4.1 æ„é€ å‚æ•°

| å‚æ•° | ç±»å‹ | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| name | str | æ˜¯ | - | Agent åç§° |
| model_client | ChatCompletionClient | æ˜¯ | - | LLM å®¢æˆ·ç«¯ |
| data_layer | AgentFusionDataLayer | æ˜¯ | - | æ•°æ®å±‚è®¿é—® |
| user_id | int | æ˜¯ | - | ç”¨æˆ· ID |
| model_context | ChatCompletionContext | å¦ | None | è‡ªå®šä¹‰ä¸Šä¸‹æ–‡ |
| system_message | str | å¦ | DEFAULT_SYSTEM_MESSAGE | è‡ªå®šä¹‰ç³»ç»Ÿæç¤º |
| max_search_iterations | int | å¦ | 3 | æœ€å¤§æœç´¢æ¬¡æ•° |

### 4.2 ç±»å¸¸é‡

| å¸¸é‡ | å€¼ | è¯´æ˜ |
|------|-----|------|
| DEFAULT_SYSTEM_MESSAGE | è§ä»£ç  | é»˜è®¤ç³»ç»Ÿæç¤º |

---

## 5. é”™è¯¯å¤„ç†

### 5.1 å¸¸è§é”™è¯¯

```python
# 1. Agent å·²åœ¨è¿è¡Œ
try:
    await agent.start()
    await agent.start()  # æŠ›å‡º ValueError
except ValueError as e:
    print(f"å¯åŠ¨é”™è¯¯: {e}")

# 2. LLM è°ƒç”¨å¤±è´¥
try:
    async for event in agent.push("æŸ¥è¯¢"):
        ...
except RuntimeError as e:
    print(f"å¤„ç†é”™è¯¯: {e}")

# 3. æ•°æ®å±‚é”™è¯¯
# å·¥å…·å‡½æ•°å†…éƒ¨å¤„ç†ï¼Œè¿”å› is_error=True çš„ç»“æœ
```

### 5.2 é‡è¯•ç­–ç•¥

```python
# åœ¨å·¥å…·å‡½æ•°å±‚é¢å®ç°é‡è¯•
async def search_memories_with_retry(..., max_retries=2):
    for attempt in range(max_retries):
        try:
            return await search_memories_tool(...)
        except TransientError:
            if attempt == max_retries - 1:
                raise
            await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 ä¸Šä¸‹æ–‡ç®¡ç†

- ä½¿ç”¨ `UnboundedChatCompletionContext`ï¼ˆé»˜è®¤ï¼‰
- æ¯æ¬¡è°ƒç”¨ `on_reset` æ—¶æ¸…ç©ºä¸Šä¸‹æ–‡
- æ”¯æŒè‡ªå®šä¹‰ä¸Šä¸‹æ–‡å®ç°

### 6.2 å¹¶å‘æ§åˆ¶

- å•æ¬¡å·¥å…·è°ƒç”¨ä¸²è¡Œæ‰§è¡Œï¼ˆä¿æŒé¡ºåºï¼‰
- å•æ¬¡è¿­ä»£ä¸­å¤šä¸ªå·¥å…·è°ƒç”¨å¯å¹¶å‘ï¼ˆå¦‚æœéœ€è¦ï¼‰

### 6.3 èµ„æºé™åˆ¶

- `max_search_iterations` é˜²æ­¢æ— é™å¾ªç¯
- æ•°æ®å±‚æŸ¥è¯¢è¶…æ—¶æ§åˆ¶
- LLM è°ƒç”¨è¶…æ—¶æ§åˆ¶

---

## 7. æµ‹è¯•ç­–ç•¥

### 7.1 å•å…ƒæµ‹è¯•

```python
import pytest
from unittest.mock import AsyncMock, MagicMock

@pytest.mark.asyncio
async def test_mem_recall_agent_basic():
    # Mock ä¾èµ–
    mock_model = AsyncMock()
    mock_model.create_stream.return_value = [
        CreateResult(content="å·¥å…·è°ƒç”¨ç»“æœ", ...)
    ]
    mock_data = MagicMock()

    # åˆ›å»º Agent
    agent = MemRecallAgent(
        name="test",
        model_client=mock_model,
        data_layer=mock_data,
        user_id=1
    )

    # æµ‹è¯•
    await agent.start()
    results = []
    async for event in agent.push("æµ‹è¯•æŸ¥è¯¢"):
        results.append(event)

    # éªŒè¯
    assert len(results) > 0
    assert any(isinstance(e, HandoffMessage) for e in results)
```

### 7.2 é›†æˆæµ‹è¯•

```python
@pytest.mark.asyncio
async def test_mem_recall_integration():
    # ä½¿ç”¨çœŸå®æ•°æ®å±‚ï¼ˆæµ‹è¯•æ•°æ®åº“ï¼‰
    data_layer = AgentFusionDataLayer("sqlite:///test.db")

    # æ’å…¥æµ‹è¯•æ•°æ®
    await data_layer.memory.store_memory(
        user_id=1,
        memory_key="test_config",
        content="æµ‹è¯•é…ç½®å†…å®¹",
        summary="æµ‹è¯•é…ç½®"
    )

    # è¿è¡Œ Agent
    agent = MemRecallAgent(...)
    # ... éªŒè¯æœç´¢ç»“æœ
```

---

## 8. æ‰©å±•æŒ‡å—

### 8.1 æ·»åŠ æ–°å·¥å…·

1. åœ¨ `memrecall_agent_tools.md` ä¸­å®šä¹‰å·¥å…·å‡½æ•°
2. åœ¨ `MEMRECALL_TOOLS` ä¸­æ³¨å†Œ
3. åœ¨ `_execute_single_tool` ä¸­æ·»åŠ æ‰§è¡Œé€»è¾‘

### 8.2 è‡ªå®šä¹‰ç§»äº¤é€»è¾‘

```python
class CustomMemRecallAgent(MemRecallAgent):
    async def _create_handoff_response(self, exec_results, model_result):
        # è‡ªå®šä¹‰ç§»äº¤é€»è¾‘
        response = await super()._create_handoff_response(exec_results, model_result)
        if response:
            # æ·»åŠ é¢å¤–ä¸Šä¸‹æ–‡
            response.content += "\n\n[è‡ªå®šä¹‰ä¿¡æ¯]"
        return response
```

### 8.3 è‡ªå®šä¹‰æ¶ˆæ¯å¤„ç†å™¨

```python
class LoggingMemRecallAgent(MemRecallAgent):
    async def handle_thought(self, message: ThoughtEvent) -> None:
        # è®°å½•æ‰€æœ‰æ€è€ƒè¿‡ç¨‹
        logger.info(f"Agent æ€è€ƒ: {message.content}")
        await super().handle_thought(message)
```

---

## 9. éƒ¨ç½²å»ºè®®

1. **èµ„æºéš”ç¦»**: MemRecallAgent åº”è¯¥è¿è¡Œåœ¨ä¸ä¸» Agent ç›¸åŒçš„è¿›ç¨‹ä¸­ï¼ˆä½å»¶è¿Ÿï¼‰
2. **è¿æ¥æ± **: ç¡®ä¿ data_layer ä½¿ç”¨è¿æ¥æ± ç®¡ç†æ•°æ®åº“è¿æ¥
3. **è¶…æ—¶é…ç½®**: è®¾ç½®åˆç†çš„ LLM è°ƒç”¨è¶…æ—¶ï¼ˆ30-60 ç§’ï¼‰
4. **ç›‘æ§**: è®°å½•æœç´¢æ¬¡æ•°ã€å»¶è¿Ÿã€æˆåŠŸç‡ç­‰æŒ‡æ ‡
